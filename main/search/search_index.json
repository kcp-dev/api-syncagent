{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation","text":"<p>The api-syncagent is a Kubernetes agent responsible for integrating external Kubernetes clusters. It runs on a Kubernetes cluster, is configured with credentials to a kcp instance and will then synchronize data out of kcp (i.e. out of kcp workspaces) onto the local cluster, and vice versa.</p>"},{"location":"#high-level-overview","title":"High-level Overview","text":"<p>The intended usecase follows roughly these steps:</p> <ol> <li>A user in kcp with sufficient permissions creates an <code>APIExport</code> object and provides appropriate    credentials for the Sync Agent (e.g. by creating a Kubernetes Secret with a preconfigured kubeconfig    in it).</li> <li>A service owner will now take these credentials and the configured API group (the <code>APIExport</code>'s    name) and use them to setup the Sync Agent. It is assumed that the service owner (i.e. the    cluster-admin in a service cluster) wants to make some resources (usually CRDs) available to use    inside of kcp.</li> <li>The service owner uses the Sync Agent Helm chart (or similar deployment technique) to install the    Sync Agent in their cluster.</li> <li>To actually make resources available in kcp, the service owner now has to create a set of    <code>PublishedResource</code> objects. The configuration happens from their point of view, meaning they    define how to publish a CRD to kcp, defining renaming rules and other projection settings.</li> <li>Once a <code>PublishedResource</code> is created in the service cluster, the Sync Agent will pick it up,    find the referenced CRD, convert/project this CRD into an <code>APIResourceSchema</code> (ARS) for kcp and    then create the ARS in org workspace.</li> <li>Finally the Sync Agent will take all <code>PublishedResources</code> and bundle them into the pre-existing    <code>APIExport</code> in the org workspace. This APIExport can then be bound in the org workspace itself    (or later any workspaces (depending on permissions)) and be used there.</li> <li>kcp automatically provides a virtual workspace for the <code>APIExport</code> and this is what the Sync Agent    then uses to watch all objects for the relevant resources in kcp (i.e. in all workspaces).</li> <li>The Sync Agent will now begin to synchronize objects back and forth between the service cluster    and kcp.</li> </ol>"},{"location":"#details","title":"Details","text":""},{"location":"#data-flow-direction","title":"Data Flow Direction","text":"<p>It might be a bit confusing at first: The <code>PublishedResource</code> CRD describes the world from the standpoint of a service owner, i.e. a person or team that owns a Kubernetes cluster and is tasked with making their CRDs available in kcp (i.e. \"publish\" them).</p> <p>However the actual data flow later will work in the opposite direction: users creating objects inside their kcp workspaces serve as the source of truth. From there they are synced down to the service cluster, which is doing the projection of the <code>PublishedResource</code> in reverse.</p> <p>Of course additional, auxiliary (related) objects could originate on the service cluster. For example if you create a Certificate object in a kcp workspace and it's synced down, cert-manager will then acquire the certificate and create a Kubernetes <code>Secret</code>, which will have to be synced back up (into a kcp workspace, where the certificate originated from). So the source of truth can also be, for auxiliary resources, on the service cluster.</p>"},{"location":"#sync-agent-naming","title":"Sync Agent Naming","text":"<p>Each Sync Agent must have a name, like \"nora\" or \"oskar\". The FQ name for a Sync Agent is <code>&lt;agentname&gt;.&lt;apigroup&gt;</code>, so if the user in kcp had created a new <code>APIExport</code> named <code>databases.examplecorp</code>, the name of the Sync Agent that serves this Service (sic) could be <code>nora.databases.examplecorp</code>.</p>"},{"location":"#uniqueness","title":"Uniqueness","text":"<p>A single <code>APIExport</code> in kcp must only be processed by exactly 1 Sync Agent. There is currently no mechanism planned to subdivide an <code>APIExport</code> into shards, where multiple service clusters (and therefore multiple Sync Agents) could process each shard.</p> <p>Later the Sync Agent might be extended with Label Selectors, alternatively they might also \"claim\" any object by annotating it in the kcp workspace. These things are not yet worked out, so for now we have this 1:1 restriction.</p> <p>Sync Agents make use of leader election, so it's perfectly fine to have multiple Sync Agent replicas, as long as only one them is leader and actually doing work.</p>"},{"location":"#kcp-awareness","title":"kcp-awareness","text":"<p>controller-runtime can be used in a \"kcp-aware\" mode, where the cache, clients, mappers etc. are aware of the workspace information. This however is neither well tested upstream and the code would require shard-admin permissions to behave like this work regular kcp workspaces. The controller-runtime fork's kcp-awareness is really more geared towards working in virtual workspaces.</p> <p>Because of this the Sync Agent needs to get a kubeconfig to kcp that already points to the <code>APIExport</code>'s workspace (i.e. the <code>server</code> URL already contains a <code>/clusters/root:myorg</code> path). The basic controllers in the Sync Agent then treat this as a plain ol', regular Kubernetes cluster (no kcp-awareness).</p> <p>To this end, the Sync Agent will, upon startup, try to access the <code>cluster</code> object in the target workspace. This is to resolve the cluster name (e.g. <code>root:myorg</code>) into a logicalcluster name (e.g. <code>gibd3r1sh</code>). The Sync Agent has to know which logicalcluster the target workspace represents in order to query resources properly.</p> <p>Only the controllers that are later responsible for interacting with the virtual workspace are kcp-aware. They have to be in order to know what workspace a resource is living in.</p>"},{"location":"#publishedresources","title":"PublishedResources","text":"<p>A <code>PublishedResource</code> describes which CRD should be made available inside kcp. The CRD name can be projected (i.e. renamed), so a <code>kubermatic.k8c.io/v1 Cluster</code> can become a <code>cloud.examplecorp/v1 KubernetesCluster</code>.</p> <p>In addition to projecting (mapping) the GVK, the <code>PublishedResource</code> also contains optional naming rules, which influence how the local objects that the Sync Agent is creating are named.</p> <p>As a single Sync Agent serves a single service, the API group used in kcp is the same for all <code>PublishedResources</code>. It's the API group configured in the <code>APIExport</code> inside kcp (created in step 1 in the overview above).</p> <p>To prevent chaos, <code>PublishedResources</code> are immutable: handling the case that a PR first wants to publish <code>kubermatic.k8c.io/v1 Cluster</code> and then suddenly <code>kubermatic.k8c.io/v1 User</code> resources would mean to re-sync and cleanup everything in all affected kcp workspaces. The Sync Agent would need to be able to delete and recreate objects to follow this GVK change, which is a level of complexity we simply do not want to deal with at this point in time. Also, <code>APIResourceSchemas</code> are immutable themselves.</p> <p>More information is available in the Publishing Resources guide.</p>"},{"location":"#apiexports","title":"APIExports","text":"<p>An <code>APIExport</code> in kcp combines multiple <code>APIResourceSchemas</code> (ARS). Each ARS is created based on a <code>PublishedResource</code> in the service cluster.</p> <p>To prevent data loss, ARS are never removed from an <code>APIExport</code>. We simply do not have enough experience to really know what happens when an ARS would suddenly become unavailable. To prevent damage and confusion, the Sync Agent will only ever add new ARS to the one <code>APIExport</code> it manages.</p>"},{"location":"#controllers","title":"Controllers","text":"<p>The Sync Agent consists of a number of independent controllers.</p>"},{"location":"#apiexport","title":"apiexport","text":"<p>This controller aggregates the <code>PublishedResources</code> and manages a single <code>APIExport</code> in kcp.</p>"},{"location":"#apiresourceschema","title":"apiresourceschema","text":"<p>This controller takes <code>PublishedResources</code>, projects and converts them and creates <code>APIResourceSchemas</code> in kcp.</p>"},{"location":"#syncmanager","title":"syncmanager","text":"<p>This controller watches the <code>APIExport</code> and waits for the virtual workspace to become available. It also watches all <code>PublishedResources</code> (PRs) and reconciles when any of them is changed (they are immutable, but the controller is still reacting to any events on them).</p> <p>The controller will then setup a controller-runtime <code>Cluster</code> abstraction for the virtual workspace and then start many <code>sync</code> controllers (one for each <code>PublishedResource</code>). Whenever PRs change, the syncmanager will make sure that the correct set of <code>sync</code> controller is running.</p>"},{"location":"#sync","title":"sync","text":"<p>This is where the meat and potatoes happen. The sync controller is started for a single <code>PublishedResource</code> and is responsible for synchronizing all objects for that resource between the local service cluster and kcp.</p> <p>The <code>sync</code> controller was written to handle a single <code>PublishedResource</code> so that it does not have to deal with dynamically registering/stopping watches on its own. Instead the sync controller can be written as more or less \"normal\" controller-runtime controller.</p>"},{"location":"consuming-services/","title":"Consuming Services","text":"<p>This document describes how to use (consume) services offered by a Sync Agent.</p>"},{"location":"consuming-services/#background","title":"Background","text":"<p>A \"service\" defines a unique Kubernetes API Group and offers a number of resources (types) to use. A service could offer certificate management, databases, cloud infrastructure or any other set of Kubernetes resources.</p> <p>Services are provided by service owners, who run their own Kubernetes clusters and take care of the maintenance and scaling tasks for the workload provisioned by all users of the service(s) they offer.</p> <p>A Service provided by a Sync Agent should not be confused with a Kubernetes Service. Internally, a \"Sync Agent Service\" is ultimately translated into a kcp <code>APIExport</code> with a number of <code>APIResourceSchemas</code> (which are more or less equivalent to CRDs).</p>"},{"location":"consuming-services/#consuming-a-service","title":"Consuming a Service","text":"<p>To consume a service (or to make use of an <code>APIExport</code>) you have to create an <code>APIBinding</code> object in the kcp workspace where the service should be used. This section assumes that you are familiar with kcp on the command line and have the kcp kubectl plugin installed.</p> <p>First you need to get the kubeconfig for accessing your kcp workspaces. Once you have set your kubeconfig up, make sure you're in the correct namespace by using <code>kubectl ws &lt;path to your workspace&gt;</code>. Use <code>kubectl ws .</code> if you're unsure where you're at.</p> <p>To enable a Service, use <code>kcp bind apiexport</code> and specify the path to and name of the <code>APIExport</code>.</p> <pre><code># kubectl kcp bind apiexport &lt;path to APIExport&gt;:&lt;API Group of the Service&gt;\nkubectl kcp bind apiexport :root:my-org:my.fancy.api\n</code></pre> <p>Without the plugin, you can create an <code>APIBinding</code> manually, simply <code>kubectl apply</code> this:</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: my.fancy.api\nspec:\n  reference:\n    export:\n      name: my.fancy.api\n      path: root:my-org\n</code></pre> <p>Shortly after, the new API will be available in the workspace. Check via <code>kubectl api-resources</code>. You can now create objects for types in that API group to your liking and they will be synced and processed behind the scenes.</p> <p>Note that a Service often has related resources, often Secrets and ConfigMaps. You must explicitly allow the Service to access these in your workspace and this means editing/patching the <code>APIBinding</code> object (the kcp kubectl plugin currently has no support for managing permission claims). For each of the claimed resources, you have to accept or reject them:</p> <pre><code>spec:\n  permissionClaims:\n    # Nearly all Sync Agents require access to namespaces, rejecting this will\n    # most likely break the Service, even more than rejecting any other claim.\n    - all: true\n      resources: namespaces\n      state: Accepted\n    - all: true\n      resources: secrets\n      state: Accepted # or Rejected\n</code></pre> <p>Rejecting a claim will severely impact a Service, if not even break it. Consult with the Service's documentation or the service owner if rejecting a claim is supported.</p> <p>When you change into (<code>kubctl ws \u2026</code>) a different workspace, kubectl will inform you if there are outstanding permission claims that you need to accept or reject.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#can-i-run-multiple-sync-agents-on-the-same-service-cluster","title":"Can I run multiple Sync Agents on the same service cluster?","text":"<p>Yes, absolutely, however you must configure them properly:</p> <p>A given <code>PublishedResource</code> must only ever be processed by a single Sync Agent Pod. The Helm chart configures leader-election by default, so you can scale up to have Pods on stand-by if needed.</p> <p>By default the Sync Agent will discover and process all <code>PublishedResources</code> in your cluster. Use the <code>--published-resource-selector</code> (<code>publishedResourceSelector</code> in the Helm values.yaml) to restrict an Agent to a subset of published resources.</p>"},{"location":"faq/#can-i-synchronize-multiple-kcp-setups-onto-the-same-service-cluster","title":"Can I synchronize multiple kcp setups onto the same service cluster?","text":"<p>Only if you have distinct API groups (and therefore also distinct <code>PublishedResources</code>) for them. You cannot currently publish the same API group onto multiple kcp setups. See issue #13 for more information.</p>"},{"location":"faq/#can-i-have-additional-resources-in-apiexports-unmanaged-by-the-sync-agent","title":"Can I have additional resources in APIExports, unmanaged by the Sync Agent?","text":"<p>Yes, you can. The agent will only ever change those resourceSchemas that match group/resource of the configured <code>PublishedResources</code>. So if you configure the agent to publish <code>cert-manager.io/Certificate</code>, this would \"claim\" all resource schemas ending in <code>.certificates.cert-manager.io</code>. When updating the <code>APIExport</code>, the agent will only touch schemas with this suffix and leave all others alone.</p> <p>This is also used when a <code>PublishedResource</code> is deleted: Since the <code>APIResourceSchema</code> remains in kcp, but is no longer configured in the agent, the agent will simply ignore the schema in the <code>APIExport</code>. This allows for async cleanup processes to happen before an admin ultimately removes the old schema from the <code>APIExport</code>.</p>"},{"location":"faq/#does-the-sync-agent-handle-permission-claims","title":"Does the Sync Agent handle permission claims?","text":"<p>Only those required for its own operation. If you configure a namespaced resource to sync, it will automatically add a claim for <code>namespaces</code> in kcp, plus it will add either <code>configmaps</code> or <code>secrets</code> if related resources are configured in a <code>PublishedResource</code>. But you cannot specify additional permissions claims.</p>"},{"location":"faq/#i-am-seeing-errors-in-the-agent-logs-whats-going-on","title":"I am seeing errors in the agent logs, what's going on?","text":"<p>Errors like</p> <p>reflector.go:561] k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list example.com/v1, Kind=Dummy: the server could not find the requested resource</p> <p>or</p> <p>reflector.go:158] \"Unhandled Error\" err=\"k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: Failed to watch kcp.example.com/v1, Kind=Dummy: failed to list kcp.example.com/v1, Kind=Dummy: the server could not find the requested resource\" logger=\"UnhandledError\"</p> <p>are typical when bootstrapping new APIExports in kcp. They are only cause for concern if they persist after configuring all PublishedResources.</p>"},{"location":"getting-started/","title":"Getting Started with the Sync Agent","text":"<p>All that is necessary to run the Sync Agent is a running Kubernetes cluster (for testing you can use kind) and a kcp installation.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster to run the Sync Agent in.</li> <li>A running kcp installation as the source of truth.</li> <li>A kubeconfig with admin or comparable permissions in a specific kcp workspace.</li> </ul>"},{"location":"getting-started/#apiexport-setup","title":"APIExport Setup","text":"<p>Before installing the Sync Agent it is necessary to create an <code>APIExport</code> on kcp. The <code>APIExport</code> should be empty, because it is updated later by the Sync Agent, but it defines the new API group we're introducing. An example file could look like this:</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIExport\nmetadata:\n  name: test.example.com\nspec: {}\n</code></pre> <p>Create a file with a similar content (you most likely want to change the name, as that is the API group under which your published resources will be made available) and create it in a kcp workspace of your choice:</p> <pre><code># use the kcp kubeconfig\n$ export KUBECONFIG=/path/to/kcp.kubeconfig\n\n# nativagate to the workspace where the APIExport should exist\n$ kubectl ws :workspace:you:want:to:create:it\n\n# create it\n$ kubectl create --filename apiexport.yaml\napiexport/test.example.com created\n</code></pre>"},{"location":"getting-started/#optional-create-initial-apibinding","title":"Optional: Create Initial APIBinding","text":"<p>To save resources, kcp doesn't start API endpoints for <code>APIExports</code> that are not in use (i.e. that don't have an active <code>APIBinding</code>). To avoid cryptic errors in the Sync Agent logs about resources not being found, you can create an initial <code>APIBinding</code> in the same (or another) workspace as your <code>APIExport</code>.</p> <p>It could look like this:</p> <pre><code>apiVersion: apis.kcp.io/v1alpha1\nkind: APIBinding\nmetadata:\n  name: test.example.com\nspec:\n  reference:\n    export:\n      name: test.example.com\n</code></pre> <p>While still being in your <code>:workspace:you:want:to:create:it</code> workspace, you could create the <code>APIBinding</code> like this:</p> <pre><code>$ kubectl create --filename apibinding.yaml\napibinding/test.example.com created\n</code></pre>"},{"location":"getting-started/#sync-agent-installation","title":"Sync Agent Installation","text":"<p>The Sync Agent can be installed into any namespace, but in our example we are going with <code>kcp-system</code>. It doesn't necessarily have to live in the same Kubernetes cluster where it is synchronizing data to, but that is the common setup. Ultimately the Sync Agent synchronizes data between two kube endpoints.</p> <p>Now that the <code>APIExport</code> is created, switch to the Kubernetes cluster from which you wish to publish resources. You will need to ensure that a kubeconfig with access to the kcp workspace that the <code>APIExport</code> has been created in is stored as a <code>Secret</code> on this cluster. Make sure that the kubeconfig points to the right workspace (not necessarily the <code>root</code> workspace).</p> <p>This can be done via a command like this:</p> <pre><code>$ kubectl create secret generic kcp-kubeconfig \\\n  --namespace kcp-system \\\n  --from-file \"kubeconfig=admin.kubeconfig\"\n</code></pre>"},{"location":"getting-started/#helm-chart-setup","title":"Helm Chart Setup","text":"<p>The Sync Agent is shipped as a Helm chart and to install it, the next step is preparing a <code>values.yaml</code> file for the Sync Agent Helm chart. We need to pass the target <code>APIExport</code>, a name for the Sync Agent itself and a reference to the kubeconfig secret we just created.</p> <pre><code># Required: the name of the APIExport in kcp that this Sync Agent is supposed to serve.\napiExportName: test.example.com\n\n# Required: This Agent's public name, used to signal ownership over locally synced objects.\n# This value must be a valid Kubernetes label value, see\n# https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set\n# for more information.\n# Changing this value after the fact will make the agent ignore previously created objects,\n# so beware and relabel if necessary.\nagentName: unique-test\n\n# Required: Name of the Kubernetes Secret that contains a \"kubeconfig\" key,\n# with the kubeconfig provided by kcp to access it.\nkcpKubeconfig: kcp-kubeconfig\n</code></pre> <p>Once this <code>values.yaml</code> file is prepared, install a recent development build of the Sync Agent:</p> <pre><code>helm repo add kcp https://kcp-dev.github.io/helm-charts\nhelm repo update\n\nhelm install kcp-api-syncagent kcp/api-syncagent \\\n  --values values.yaml \\\n  --namespace kcp-system\n</code></pre> <p>Two <code>kcp-api-syncagent</code> Pods should start in the <code>kcp-system</code> namespace. If they crash you will need to identify the reason from container logs. A possible issue is that the provided kubeconfig does not have permissions against the target kcp workspace.</p>"},{"location":"getting-started/#service-cluster-rbac","title":"Service Cluster RBAC","text":"<p>The Sync Agent usually requires additional RBAC on the service cluster to function properly. The Helm chart will automatically allow it to read CRDs, namespaces and Secrets, but depending on how you configure your PublishedResources, additional permissions need to be created.</p> <p>For example, if the Sync Agent is meant to create <code>Certificate</code> objects (defined by cert-manager), you would need to grant it permissions on those:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: 'api-syncagent:unique-test'\nrules:\n  - apiGroups:\n      - cert-manager.io\n    resources:\n      - certificates\n    verbs:\n      - get\n      - list\n      - watch\n      - create\n      - update\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: 'api-syncagent:unique-test'\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: 'api-syncagent:unique-test'\nsubjects:\n  - kind: ServiceAccount\n    name: 'kcp-api-syncagent'\n    namespace: kcp-system\n</code></pre> <p>NB: Even though the PublishedResources might only create/update Certificates in a single namespace, due to the inner workings of the Agent they will still be watched (cached) cluster-wide. So you can tighten permissions on <code>create</code>/<code>update</code> operations to certain namespaces, but <code>watch</code> permissions need to be granted cluster-wide.</p>"},{"location":"getting-started/#kcp-rbac","title":"kcp RBAC","text":"<p>The Helm chart is installed on the service cluster and so cannot provision the necessary RBAC for the Sync Agent within kcp. Usually whoever creates the <code>APIExport</code> is also responsible for creating the RBAC rules that grant the Agent access.</p> <p>The Sync Agent needs to</p> <ul> <li>access the workspace of its <code>APIExport</code>,</li> <li>get the <code>LogicalCluster</code>,</li> <li>manage its <code>APIExport</code>,</li> <li>manage <code>APIResourceSchemas</code> and</li> <li>access the virtual workspace for its <code>APIExport</code>.</li> </ul> <p>This can be achieved by applying RBAC like this in the workspace where the <code>APIExport</code> resides:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: api-syncagent-mango\nrules:\n  # get the LogicalCluster\n  - apiGroups:\n      - core.kcp.io\n    resources:\n      - logicalclusters\n    resourceNames:\n      - cluster\n    verbs:\n      - get\n  # manage its APIExport\n  - apiGroups:\n      - apis.kcp.io\n    resources:\n      - apiexports\n    resourceNames:\n      - test.example.com\n    verbs:\n      - get\n      - list\n      - watch\n      - patch\n      - update\n  # manage APIResourceSchemas\n  - apiGroups:\n      - apis.kcp.io\n    resources:\n      - apiresourceschemas\n    verbs:\n      - get\n      - list\n      - watch\n      - create\n  # access the virtual workspace\n  - apiGroups:\n      - apis.kcp.io\n    resources:\n      - apiexports/content\n    resourceNames:\n      - test.example.com\n    verbs:\n      - '*'\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: api-syncagent-mango:system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: api-syncagent-mango\nsubjects:\n  - kind: User\n    name: api-syncagent-mango\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: api-syncagent-mango:access\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:kcp:workspace:access\nsubjects:\n  - kind: User\n    name: api-syncagent-mango\n</code></pre>"},{"location":"getting-started/#publish-resources","title":"Publish Resources","text":"<p>Once the Sync Agent Pods are up and running, you should be able to follow the Publishing Resources guide.</p>"},{"location":"getting-started/#consume-service","title":"Consume Service","text":"<p>Once resources have been published through the Sync Agent, they can be consumed on the kcp side (i.e. objects on kcp will be synced back and forth with the service cluster). Follow the guide to consuming services.</p>"},{"location":"contributing/releasing/","title":"Release Process","text":"<p>The guide describes how to release a new version of the api-syncagent.</p>"},{"location":"contributing/releasing/#prerequisites","title":"Prerequisites","text":"<ol> <li>Have all desired changes merged and/or cherrypicked into the appropriate    release branch.</li> </ol>"},{"location":"contributing/releasing/#minor-release","title":"Minor Release","text":"<p>Minor releases (0.x) are tagged directly on the <code>main</code> branch and the <code>v0.X.0</code> tag represents where the corresponding <code>release/v0.X</code> branch branches off.</p> <ol> <li>Checkout the desired <code>main</code> branch commit.</li> <li>Tag the main module: <code>git tag -m \"version 0.X\" v0.X.0</code></li> <li>Tag the SDK module: <code>git tag -m \"SDK version 0.X\" sdk/v0.X.0</code></li> <li>Push the tags: <code>git push upstream v0.X.0 sdk/v0.X.0</code></li> <li>Create the release branch: <code>git checkout -B release/v0.X</code></li> <li>Push the release branch: <code>git push -u upstream release/v0.X</code></li> </ol>"},{"location":"contributing/releasing/#patch-releases","title":"Patch Releases","text":"<p>Patch releases (v0.x.y) are tagged with in a release branch.</p> <ol> <li>Checkout the desired <code>release/v0.X</code> branch commit.</li> <li>Tag the main module: <code>git tag -m \"version 0.X.Y\" v0.X.Y</code></li> <li>Tag the SDK module: <code>git tag -m \"SDK version 0.X.Y\" sdk/v0.X.Y</code></li> <li>Push the tags: <code>git push upstream v0.X.Y sdk/v0.X.Y</code></li> </ol>"},{"location":"publish-resources/","title":"Macro Syntax Error","text":"<p>File: <code>publish-resources/index.md</code></p> <p>Line 150 in Markdown file: unexpected '.' <pre><code>    namespace: '{{ .ClusterName }}'\n</code></pre></p>"},{"location":"publish-resources/api-lifecycle/","title":"API Lifecycle","text":"<p>In only the rarest of cases will the first version of a CRD be also its final version. Instead usually CRDs evolve over time and Kubernetes has strong, though sometimes hard to use, support for managing different versions of CRDs and their resources.</p> <p>To understand how CRDs work in the context of the Sync Agent, it's important to first get familiar with the regular Kubernetes behaviour regarding CRD versioning.</p>"},{"location":"publish-resources/api-lifecycle/#basics","title":"Basics","text":"<p>The Sync Agent will, whenever a published CRD changes (this can also happen when the projection rules inside a <code>PublishedResource</code> are updated), create a new <code>APIResourceSchema</code> (ARS) in kcp. The name and version of this ARS are based on a hash of the projected CRD. Undoing a change would make the agent re-use the previously created ARS (ARS are immutable).</p> <p>After every reconciliation, the list of latest resource schemas in the configured <code>APIExport</code> is updated. For this the agent will find all ARS that belong to it (based on an ownership label) and then merge them into the <code>APIExport</code>. Resource schemas for unknown group/resource combinations are left untouched, so admins are free to add additional resource schemas to an <code>APIExport</code>.</p> <p>This means that every change to a CRD on the service cluster is applied practically immediately in each workspace that consumes the <code>APIExport</code>. Administrators are wise to act carefully when working with their CRDs on their service cluster. Sometimes it can make sense to turn-off the agent before testing new CRDs, even though this will temporarily suspend the synchronization.</p>"},{"location":"publish-resources/api-lifecycle/#single-version-crds","title":"Single-Version CRDs","text":"<p>A very common scenario is to only ever have a single version inside each CRD and keeping this version perpetually backwards-compatible. As long as all consumers are aware that certain fields might not be set yet in older objects, this scheme works out generally fine.</p> <p>The agent will handle this scenario just fine by itself. Whenever a CRD is updated, it will reflect those changes back into a new <code>APIResourceSchema</code> and update the <code>APIExport</code>, making the changes immediately available to all consumers. Since the agent itself doesn't much care for the contents of objects, it itself is not affected by any structural changes in CRDs, as long as it is able to apply them on the underlying Kubernetes cluster.</p>"},{"location":"publish-resources/api-lifecycle/#multi-version-crds","title":"Multi-Version CRDs","text":"<p>Having multiple versions in a single CRD is immediately much more work, since in Kubernetes all versions of a CRD must be losslessly convertible to every other version. Without CEL expressions or a dedicated conversion webhook this is practically impossible to achieve.</p> <p>At the moment kcp does not support CEL-based conversions, and there is no support for configuring a conversion webhook inside the Sync Agent either. This is because such a webhook would need to run very close to the kcp shards and it's simply out of scope for such a component to be described and deployed by the Sync Agent, let alone a trust nightmare for the kcp operators who would have to run foreign webhooks on their cluster.</p> <p>Since both conversion mechanisms are not usable in the current state of kcp and the Sync Agent, having multiple versions in a CRD can be difficult to manage.</p> <p>Generally the Sync Agent itself does not care much about the schemas of each CRD version or the convertibility between them. The synchronization works by using unstructured clients to the storage versison of the CRD on both sides (in kcp and on the service cluster). Which version is the storage version is up to the CRD author.</p> <p>When publishing multiple versions of a CRD</p> <ul> <li>only those versions marked as <code>served</code> can be picked and</li> <li>if no <code>storage</code> version is picked, the latest (highest) version will be chosen automatically as   the storage version in kcp.</li> </ul>"},{"location":"publish-resources/technical-details/","title":"Technical Details","text":"<p>The following sections go into more details of the behind the scenes magic.</p>"},{"location":"publish-resources/technical-details/#synchronization","title":"Synchronization","text":"<p>Even though the whole configuration is written from the standpoint of the service owner, the actual synchronization logic considers the kcp side as the canonical source of truth. The Sync Agent continuously tries to make the local objects look like the ones in kcp, while pushing status updates back into kcp (if the given <code>PublishedResource</code> (i.e. CRD) has a <code>status</code> subresource enabled).</p>"},{"location":"publish-resources/technical-details/#local-remote-connection","title":"Local &lt;-&gt; Remote Connection","text":"<p>The Sync Agent tries to keep sync-related metadata on the service cluster, away from the consumers. This is both to prevent vandalism and to hide implementation details.</p> <p>To ensure stability against future changes, once the Sync Agent has determined how a local object should be named, it will remember this decision in the object's metadata. This is so that on future reconciliations, the (potentially costly, but probably not) renaming logic does not need to be applied again. This allows the Sync Agent to change defaults and also allows the service owner to make changes to the naming rules without breaking existing objects.</p> <p>Since we do not want to store metadata on the kcp side, we instead rely on label selectors on the local objects. Each object on the service cluster has a label for the remote cluster name, namespace and object name, and when trying to find the matching local object, the Sync Agent simply does a label-based search.</p> <p>There is currently no sync-related metadata available on source objects (in kcp workspaces), as this would either be annotations (untyped strings...) or require schema changes to allow additional fields in basically random CRDs.</p> <p>Note that fields like <code>generation</code> or <code>resourceVersion</code> are not relevant for any of the sync logic.</p>"},{"location":"publish-resources/technical-details/#reconcile-loop","title":"Reconcile Loop","text":"<p>The sync loop can be divided into 5 parts:</p> <ol> <li>find the local object</li> <li>handle deletion</li> <li>ensure the destination object exists</li> <li>ensure the destination object's content matches the source object</li> <li>synchronize related resources the same way (repeat 1-4 for each related resource)</li> </ol>"},{"location":"publish-resources/technical-details/#phase-1-find-the-local-object","title":"Phase 1: Find the Local Object","text":"<p>For this, as mentioned in the connection chapter above, the Sync Agent tries to follow label selectors on the service cluster. This helps prevent cluttering with consumer workspaces with sync metadata. If no object is found to match the labels, that's fine and the loop will continue with phase 2, in which a possible Conflict error (if labels broke) is handled gracefully.</p> <p>The remote object in the workspace becomes the <code>source object</code> and its local equivalent on the service cluster is called the <code>destination object</code>.</p>"},{"location":"publish-resources/technical-details/#phase-2-handle-deletion","title":"Phase 2: Handle Deletion","text":"<p>A finalizer is used in the kcp workspaces to prevent orphans in the service cluster side. This is the only real evidence in the kcp side that the Sync Agent is even doing things. When a remote (source) object is deleted, the corresponding local object is deleted as well. Once the local object is gone, the finalizer is removed from the source object.</p>"},{"location":"publish-resources/technical-details/#phase-3-ensure-object-existence","title":"Phase 3: Ensure Object Existence","text":"<p>We have a source object and now need to create the destination. This chart shows what's happening.</p> <pre><code>graph TB\n    A(source object):::state --&gt; B([cleanup if in deletion]):::step\n    B --&gt; C([ensure finalizer on source object]):::step\n    C --&gt; D{exists local object?}\n\n    D -- yes --&gt; I(\"continue with next phase\u2026\"):::state\n    D -- no --&gt; E([apply projection]):::step\n\n    subgraph \"ensure dest object exists\"\n    E --&gt; G([ensure resulting namespace exists]):::step\n    G --&gt; H([create local object]):::step\n    H --&gt; H_err{Errors?}\n    H_err -- Conflict --&gt; J([attempt to adopt existing object]):::step\n    end\n\n    H_err -- success --&gt; I\n    J --&gt; I\n\n    classDef step color:#77F\n    classDef state color:#F77</code></pre> <p>After we followed through with these steps, both the source and destination objects exists and we can continue with phase 4.</p> <p>Resource adoption happens when creation of the initial local object fails. This can happen when labels get mangled. If such a conflict happens, the Sync Agent will \"adopt\" the existing local object by adding / fixing the labels on it, so that for the next reconciliation it will be found and updated.</p>"},{"location":"publish-resources/technical-details/#phase-4-content-synchronization","title":"Phase 4: Content Synchronization","text":"<p>Content synchronization is rather simple, really.</p> <p>First the source \"spec\" is used to patch the local object. Note that this step is called \"spec\", but should actually be called \"all top-level elements besides <code>apiVersion</code>, <code>kind</code>, <code>status</code> and <code>metadata</code>, but still including some labels and annotations\"; so if you were to publish RBAC objects, the syncer would include <code>roleRef</code> field, for example).</p> <p>To allow proper patch generation, the last known state is kept on the local object, similar to how <code>kubectl</code> creates an annotation for it. This is required for the Sync Agent to properly detect changes made by mutation webhooks on the service cluster.</p> <p>If the published resource (CRD) has a <code>status</code> subresource enabled (not just a <code>status</code> field in its scheme, it must be a real subresource), then the Sync Agent will copy the status from the local object back up to the remote (source) object.</p>"},{"location":"publish-resources/technical-details/#phase-5-sync-related-resources","title":"Phase 5: Sync Related Resources","text":"<p>The same logic for synchronizing the main published resource applies to their related resources as well. The only difference is that the source side can be either remote (workspace) or local (service cluster).</p> <p>Since the Sync Agent tries its best to keep sync-related data out of kcp workspaces, the last known state for related resources is not kept together with the destination object in the kcp workspaces. Instead all known states (from the main object and all related resources) is kept in a single Secret on the service cluster side.</p>"},{"location":"publish-resources/templating/","title":"Macro Syntax Error","text":"<p>File: <code>publish-resources/templating.md</code></p> <p>Line 4 in Markdown file: unexpected '.' <pre><code>places. A simple template could look like `{{ .Object.spec.secretName | sha3sum }}`.\n</code></pre></p>"}]}